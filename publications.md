---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: default
title: Ikko Yamane, Postdoc Researcher
subtitle: Paris-Dauphine University/RIKEN AIP
---
<div markdown="1" class="content">

# Publications

## Conferences
- Tianyi Zhang, Ikko Yamane, Nan Lu, and Masashi Sugiyama.<br />
A One-step Approach to Covariate Shift Adaptation.<br />
In _the 12th Asian Conference on Machine Learning ([ACML 2020](http://www.acml-conf.org/2020/))_,
Proceedings of Machine Learning Research, vol.129, pp.65-80, 2020.<br />
[[ACML paper](http://proceedings.mlr.press/v129/zhang20a.html), [video](http://www.acml-conf.org/2020/video/paper/zhang20a)] ([Best Paper Award!](http://www.acml-conf.org/2020/program/awards))

- Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, and Masashi Sugiyama.<br />
Do We Need Zero Training Loss After Achieving Zero Training Error?<br />
In _the 37th International Conference on Machine Learning ([ICML 2020](https://icml.cc/Conferences/2020))_,
Proceedings of Machine Learning Research, vol.119, pp.4604-4614, 2020.<br />
[[ICML paper](http://proceedings.mlr.press/v119/ishida20a/ishida20a.pdf),
[arXiv version](https://arxiv.org/abs/2002.08709),
[code on GitHub](https://github.com/takashiishida/flooding)]

- Ikko Yamane, Florian Yger, Jamal Atif, and Masashi Sugiyama.<br />
Uplift Modeling from Separate Labels.
In _[Advances in Neural Information Processing Systems 31](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018) ([NeurIPS 2018](https://nips.cc/Conferences/2018/))_,
pp.9949-9959, 2018.<br />
[[NeurIPS paper](https://papers.nips.cc/paper/8198-uplift-modeling-from-separate-labels),
[arXiv version](https://arxiv.org/abs/1803.05112),
[code on GitHub](https://github.com/i-yamane/uplift)]

- Ikko Yamane, Florian Yger, Maxime Berar, and Masashi Sugiyama.<br />
Multitask Principal Component Analysis.<br />
In <a href="http://www.acml-conf.org/2016/"><i>the 8th Asian Conference on Machine Learning (ACML 2016)</i></a>,
Proceedings of Machine Learning Research, vol.63, pp.302-317, 2016.<br />
[[ACML paper](http://proceedings.mlr.press/v63/yamane65.pdf),
[code on GitLab](https://gitlab.com/yamane.ikko/MTPCA)]


## Journal Articles
- Ikko Yamane, Hiroaki Sasaki, and Masashi Sugiyama.<br />
Regularized Multi-Task Learning for Multi-Dimensional Log-Density Gradient Estimation.<br />
<a href="https://www.mitpressjournals.org/loi/neco"><i>Neural Computation</i></a>, vol.28, no.6, pp.1388-1410, 2016.<br />
[<a href="http://www.ms.k.u-tokyo.ac.jp/2016/MT-LSLDG.pdf">paper</a>]

- Akinori Kawachi and Ikko Yamane.<br />
A Fourier-Analytic Approach to List-Decoding for Sparse Random Linear Codes.<br />
<i>IEICE Transactions on Information and Systems</i>, vol.E98-D, no.3, pp.532-540, 2015.<br />
[<a href="https://www.jstage.jst.go.jp/article/transinf/E98.D/3/E98.D_2014FCP0016/_article">paper</a>]

</div>
